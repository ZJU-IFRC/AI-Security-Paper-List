# AI-Security-Paper-List
Track AI security papers in top conferences.

ICLR

| ID | Paper Title                                                                                                | Type | Code                                                        | Year | Source |
|----|------------------------------------------------------------------------------------------------------------|------|-------------------------------------------------------------|------|--------|
| 1  | Beyond Memorization: Violating Privacy via Inference with Large Language Models                            |      | https://github.com/eth-sri/llmprivacy                       | 2024 | ICLR   |
| 2  | JAILBREAK IN PIECES: COMPOSITIONAL ADVERSARIAL ATTACKS ON MULTI-MODAL LANGUAGE MODELS                      |      |                                                             | 2024 | ICLR   |
| 3  | CATASTROPHIC JAILBREAK OF OPEN-SOURCE LLMS VIA EXPLOITING GENERATION                                       |      | https://github.com/Princeton-SysML/Jailbreak_LLM            | 2024 | ICLR   |
| 4  | "FINE-TUNING ALIGNED LANGUAGE MODELS COMPROMISES SAFETY, EVEN WHEN USERS DO NOT INTEND TO!"               |      | https://github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety | 2024 | ICLR   |
| 5  | Confidential-PROFITT: Confidential PROof of FaIr Training of Trees                                         |      | https://github.com/cleverhanslab/Confidential-PROFITT       | 2023 | ICLR   |
| 6  | Clean-image Backdoor: Attacking Multi-label Models with Poisoned Labels Only                               |      |                                                             | 2023 | ICLR   |
| 7  | Provable Defense Against Geometric Transformations                                                         |      | https://github.com/uiuc-arc/CGT                             | 2023 | ICLR   |
| 8  | UNICORN: A Unified Backdoor Trigger Inversion Framework                                                    |      | https://github.com/RU-System-Software-and-Security/UNICORN  | 2023 | ICLR   |
| 9  | Indiscriminate Poisoning Attacks on Unsupervised Contrastive Learning                                      |      | https://github.com/kaiwenzha/contrastive-poisoning          | 2023 | ICLR   |
| 10 | Is Adversarial Training Really a Silver Bullet for Mitigating Data Poisoning?                              |      | https://github.com/WenRuiUSTC/EntF                          | 2023 | ICLR   |
| 11 | Turning the Curse of Heterogeneity in Federated Learning into a Blessing for Out-of-Distribution Detection |      | https://github.com/illidanlab/FOSTER                        | 2023 | ICLR   |

NeurIPS

| ID | Paper Title                                                                                    | Type                  | Code                                                        | Year | Source  |
|----|------------------------------------------------------------------------------------------------|-----------------------|-------------------------------------------------------------|------|---------|
| 2  | Jailbroken: How Does LLM Safety Training Fail?                                                 | Large Language Models | https://github.com/corca-ai/awesome-llm-security            | 2023 | NeurIPS |
| 3  | Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition              | Bias Mitigation       | https://github.com/dooleys/FR-NAS                           | 2023 | NeurIPS |
| 4  | Privacy Auditing with One (1) Training Run                                                     | Differential Privacy  |                                                             | 2023 | NeurIPS |
| 5  | Students Parrot Their Teachers: Membership Inference on Model Distillation                     | Privacy               |                                                             | 2023 | NeurIPS |
| 6  | User-Level Differential Privacy With Few Examples Per User                                     | Differential Privacy  |                                                             | 2023 | NeurIPS |
| 7  | Evaluating the Moral Beliefs Encoded in LLMs                                                   | Language Models       | https://github.com/ninodimontalcino/moralchoice             | 2023 | NeurIPS |
| 8  | In-Context Impersonation Reveals Large Language Models' Strengths and Biases                   | large language models | https://github.com/ExplainableML/in-context-impersonation.  | 2023 | NeurIPS |
| 9  | ProPILE: Probing Privacy Leakage in Large Language Models                                      | Large language model  |                                                             | 2023 | NeurIPS |
| 10 | Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision | Large Language Models | https://github.com/IBM/Dromedary                            | 2023 | NeurIPS |
| 11 | Differentially Private Image Classification by Learning Priors from Random Processes           | Differential Privacy  | https://github.com/inspire-group/dp-randp                   | 2023 | NeurIPS |
| 12 | Participatory Personalization in Classification                                                | Data Privacy          |                                                             | 2023 | NeurIPS |
| 13 | Stable Diffusion is Unstable                                                                   | Diffusion Model       | https://github.com/duchengbin8/Stable_Diffusion_is_Unstable | 2023 | NeurIPS |

ICCV

| ID | Paper Title                                                                                      | Type | Code                                   | Year | Source |
|----|--------------------------------------------------------------------------------------------------|------|----------------------------------------|------|--------|
| 1  | Enhancing Fine-Tuning Based Backdoor Defense with Sharpness-Aware Minimization                   |      | https://github.com/SCLBD/BackdoorBench | 2023 | ICCV   |
| 2  | The Perils of Learning From Unlabeled Data:Backdoor Attacks on Semi-supervised Learning          |      |                                        | 2023 | ICCV   |
| 3  | Privacy-Preserving Face Recognition Using Random Frequency Components                            |      | https://www.kdocs.cn/l/cibnMSXxm4mc    | 2023 | ICCV   |
| 4  | Privacy Preserving Localization via Coordinate Permutations                                      |      |                                        | 2023 | ICCV   |
| 5  | Enhancing Privacy Preservation in Federated Learning via Learning Rate Perturbation              |      |                                        | 2023 | ICCV   |
| 6  | PIRNet: Privacy-Preserving Image Restoration Network via Wavelet Lifting                         |      |                                        | 2023 | ICCV   |
| 7  | Towards Attack-tolerant Federated Learning via Critical Parameter Analysis                       |      | https://github.com/Sungwon-Han/FEDCPA  | 2023 | ICCV   |
| 8  | LEA2: A Lightweight Ensemble Adversarial Attack via Non-overlapping Vulnerable Frequency Regions |      |                                        | 2023 | ICCV   |
